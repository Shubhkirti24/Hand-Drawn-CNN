# -*- coding: utf-8 -*-
"""project_shubpras.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fVyOdSV1EtyrjblKlsVFMaR65Wxnqb4v
"""

import sys

import pandas as pd
import numpy as np
import tensorflow
from tensorflow import keras

import pickle

import sklearn
from sklearn.model_selection import train_test_split

from keras.models import Sequential
from keras.layers import Conv2D, Lambda, MaxPooling2D
from keras.layers import Dense, Dropout, Flatten 
# from tensorflow.keras.layers import BatchNormalization

from keras.preprocessing.image import ImageDataGenerator
from keras.utils.np_utils import to_categorical


def train(file) :
  df = pd.read_csv(file)
  train_data = np.array([x.reshape(28 * 28, ) for x in df['data']])
  X_main = train_data
  X_main = X_main.reshape(-1,28,28,1)
  y_check = df['target']
  y_check = to_categorical(y_check)
  X_train, X_test, y_train, y_test = train_test_split(X_main, y_check, test_size=0.1, random_state=0)
  
  model=Sequential()
  model.add(Conv2D(filters=64, kernel_size = (3,3), activation="relu", input_shape=(28,28,1)))
  model.add(Conv2D(filters=64, kernel_size = (3,3), activation="relu"))
  model.add(MaxPooling2D(pool_size=(2,2)))
  # model.add(BatchNormalization())
  model.add(Conv2D(filters=128, kernel_size = (3,3), activation="relu"))
  model.add(Conv2D(filters=128, kernel_size = (3,3), activation="relu"))
  model.add(MaxPooling2D(pool_size=(2,2)))
  # model.add(BatchNormalization())    
  model.add(Conv2D(filters=256, kernel_size = (3,3), activation="relu"))
  model.add(MaxPooling2D(pool_size=(2,2)))

  # model.add(BatchNormalization())
  model.add(Flatten())
  model.add(Dense(512,activation="relu"))

  model.add(Dense(100,activation="softmax"))
  
  
  
  model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])
  Augumented_data = ImageDataGenerator(
    rotation_range=20,  
    zoom_range = 0.01,
    width_shift_range=0.1,
    height_shift_range=0.1) 
        
  train_aug_image = Augumented_data.flow(X_train, y_train, batch_size=128)
  test_aug_image = Augumented_data.flow(X_test, y_test, batch_size=128)

  num_epochs = 50
  batch_size = 128
  train_steps = X_train.shape[0] // batch_size
  valid_steps = X_test.shape[0] // batch_size
  early_stop = keras.callbacks.EarlyStopping(
    monitor="val_accuracy",
    patience=5,
    verbose=1,
    mode="max",
    restore_best_weights=True 
    )
    
  plateau_achieved = keras.callbacks.ReduceLROnPlateau(
    monitor="val_accuracy",
    factor=0.2,
    patience=5,
    verbose=1,
    mode="max",
    min_lr=0.00001,
    )

  history = model.fit(train_aug_image, 
  epochs = num_epochs, 
  steps_per_epoch = train_steps,
  validation_data = test_aug_image,
  validation_steps = valid_steps, 
  callbacks=[early_stop, plateau_achieved])

def test(file):
  df_test = pd.read_pickle(file)
  test_data = np.array([x.reshape(28 * 28, ) for x in df_test['data']])
  test_data = test_data.reshape(-1,28,28,1)
  test_x = test_data.reshape(-1,28,28,1)
  # pred = model.predict(test_x, verbose=1)

def run(test_image):
    model = keras.models.load_model('model_shubpras.h5')
    test_data = np.array([x.reshape(28 * 28, ) for x in df['data'] ])
    test_data = test_data.reshape(test_data.shape[0],28,28)
    test_data = test_data.reshape(-1,28,28,1)
    result=model.predict(test_data, verbose=1)
    result = np.argmax(result, 1) 


    with open('project_shubpras.txt', 'w') as file:  # edit here as your username
        file.write('\n'.join(map(str, result)))
        file.flush()
        return True
    return False


if __name__ == "__main__":
    # we will run your code by the following command
    # python project_xiaoq.py argv[1] argv[2]
    # argv[1] is the path of training set
    # argv[2] is the path of test set
    # for example, python demo.py train100c5k_v2.pkl test100c5k_nolabel.pkl

    try:
        df = pd.read_pickle(sys.argv[1])  # training set path
        train_data = df['data'].values
        train_target = df['target'].values
        df = pd.read_pickle(sys.argv[2])  # test set path
        test_data = df['data'].values
        info = run(test_data)
        if not info:
            print(sys.argv[0] + ": Return False")
    except RuntimeError:
        print(sys.argv[0] + ": An RuntimeError occurred")
    except:
        print(sys.argv[0] + ": An exception occurred")